
.. _theano:

******
Theano
******

Pointers
--------

* http://deeplearning.net/software/theano/
* Announcements mailing list: http://groups.google.com/group/theano-announce
* User mailing list: http://groups.google.com/group/theano-users
* Deep Learning Tutorials: http://www.deeplearning.net/tutorial/
* Installation: https://deeplearning.net/software/theano/install.html


Description
-----------

* Mathematical symbolic expression compiler
* Dynamic C/CUDA code generation
* Efficient symbolic differentiation
 
  * Theano computes derivatives of functions with one or many inputs.

* Speed and stability optimizations

  * Gives the right answer for ``log(1+x)`` even if x is really tiny.
  
* Works on Linux, Mac and Windows
* Transparent use of a GPU

  * float32 only for now (working on other data types)
  * Still in experimental state on Windows
  * On GPU data-intensive calculations are typically between 6.5x and 44x faster. We've seen speedups up to 140x

* Extensive unit-testing and self-verification

  * Detects and diagnoses many types of errors
  
* On CPU, common machine learning algorithms are 1.6x to 7.5x faster than competitive alternatives

  * including specialized implementations in C/C++, NumPy, SciPy, and Matlab

* Expressions mimic NumPy's syntax & semantics
* Statically typed and purely functional
* Some sparse operations (CPU only)
* The project was started by James Bergstra and Olivier Breuleux
* For the past 1-2 years, I have replaced Olivier as lead contributor

Simple example
--------------

>>> import theano
>>> a = theano.tensor.vector("a")      # declare symbolic variable
>>> b = a + a**10                      # build symbolic expression
>>> f = theano.function([a], b)        # compile function
>>> print f([0,1,2])
[    0.     2.  1026.]

======================================================  =====================================================
        Unoptimized graph                                    Optimized graph
======================================================  =====================================================
.. image:: ../hpcs2011_tutorial/pics/f_unoptimized.png   .. image:: ../hpcs2011_tutorial/pics/f_optimized.png
======================================================  =====================================================

Symbolic programming = *Paradigm shift*: people need to use it to understand it.

Exercise 1
-----------

.. testcode::

  import theano
  a = theano.tensor.vector() # declare variable
  out = a + a**10               # build symbolic expression
  f = theano.function([a], out)   # compile function
  print f([0,1,2])

.. testoutput::

    [    0.     2.  1026.]

.. testcode::

  theano.printing.pydotprint_variables(b, outfile="f_unoptimized.png", var_with_name_simple=True)
  theano.printing.pydotprint(f, outfile="f_optimized.png", var_with_name_simple=True)
  
.. testoutput::

    The output file is available at f_unoptimized.png
    The output file is available at f_optimized.png

Modify and execute the example to do this expression: a**2 + b**2 + 2*a*b

Real example
------------

**Logistic Regression**

* GPU-ready
* Symbolic differentiation
* Speed optimizations
* Stability optimizations

.. testcode::

  import numpy
  import theano
  import theano.tensor as T
  rng = numpy.random
  
  N = 400
  feats = 784
  D = (rng.randn(N, feats), rng.randint(size=N,low=0, high=2))
  training_steps = 10000
  
  # Declare Theano symbolic variables
  x = T.matrix("x")
  y = T.vector("y")
  w = theano.shared(rng.randn(feats), name="w")
  b = theano.shared(0., name="b")
  print("Initial model:")
  print(w.get_value())
  print(b.get_value())

  # Construct Theano expression graph
  p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b))     # Probability that target = 1
  prediction = p_1 > 0.5                    # The prediction thresholded
  xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) # Cross-entropy loss function
  cost = xent.mean() + 0.01*(w**2).sum()    # The cost to minimize
  gw,gb = T.grad(cost, [w,b])

  # Compile
  train = theano.function(
            inputs=[x,y],
            outputs=[prediction, xent],
            updates={w:w-0.1*gw, b:b-0.1*gb})
  predict = theano.function(inputs=[x], outputs=prediction)

  # Train
  for i in range(training_steps):
      pred, err = train(D[0], D[1])

  print("Final model:")
  print(w.get_value())
  print(b.get_value())
  print("target values for D:")
  print(D[1])
  print("prediction on D:")
  print(predict(D[0]))

.. testoutput::
   :hide:
   :options: +ELLIPSIS
   
   Initial model:
   ...
   ...
   Final model:
   ...
   ...
   target values for D:
   ...
   prediction on D:
   ...

.. code-block:: none

 Initial model:
    [-0.80084477  0.08402541 -1.66860783 -1.02532268  0.8886402   0.14476805
     -1.14120829  0.02111008 -1.28930377 -1.19205588  1.52709999  0.62698583
     -1.36344437 -0.2820571  -0.62747568  1.01171942 -0.6850677   0.46914037
      1.22092361  0.09481212  0.6036099  -0.35937826 -0.59314987  1.85329242
      0.41008656  0.26418981  0.47865532 -0.80803873 -1.13747724  2.22045813
     -1.57167297 -2.05183915 -0.54262419 -0.44372797 -2.21568648 -0.87423319
      0.8886323  -0.07458182 -0.92555247  0.59130313  1.78329406  1.16349621
     -0.5988128  -0.76714526 -0.04171616  0.0196169   0.23416701 -0.97311606
     -1.022983    0.11489837  0.00466539  0.61490181  0.37961962  0.54924342
      1.81222343 -1.45416796  0.49508227 -0.09832841  0.86139477  0.60893093
     -0.6438551  -0.3906909  -1.31382051  1.40039028  0.17294133 -0.82709963
     -1.41584066 -1.94627393 -0.05828912 -0.40063652 -1.63678493 -0.57730862
      0.58096965  0.277437    0.50371749 -0.89151342 -0.12088879 -0.11074486
     -0.87117859 -0.1186343   1.70427993 -0.00722216  0.26761211 -0.24386062
     -0.13141852 -1.34404817  1.77707865  0.52259751  0.71267306 -1.42549549
     -1.05392181  0.73385483  0.13548241 -1.38491226 -1.70694017 -1.18830233
      0.09259394  0.54855189  0.77840212 -1.15892446 -1.53607787  0.54710093
      0.40542358 -0.13222428 -0.74640839  0.90172798 -0.19719775  0.49371906
      0.84635855  0.12940285 -1.75169904 -0.88786795  0.41245698 -0.51077275
     -0.91502789  0.61423383  0.82113865  0.47876647  0.30140761  0.39189472
     -0.23983979 -1.17086525  0.67619654 -0.50551203 -1.36214747  0.66380288
      0.11641786 -1.16436661  0.05182345 -1.79938783  0.76449263 -0.31994312
      0.08688681 -0.23396464 -1.21468424 -0.29180975 -0.43535094  0.54918402
     -0.55849561  0.01894498 -0.741132   -1.35758866  0.45156651  1.15917877
      0.81772458  0.23969816 -1.53713126  1.21870134 -0.07043454 -0.93895637
     -0.01893687  1.51888325 -0.57241018  1.56924311 -0.55310527  0.08377235
     -1.98586381  0.72423665  0.18641669  0.57843263 -1.78002104  1.66851035
      0.67460144  1.736838    0.02468704  0.66033703  0.17615996 -0.8833387
     -1.62523746 -0.86377071  0.512516    0.4041653  -0.97256049  0.38305227
     -0.94852792  1.54990616  0.04102654 -0.59694816  1.30360408  1.67598413
     -0.05686776  2.12721133  0.67905476  0.33876247  0.94111031 -0.76701797
      0.40245192 -1.94655518 -1.06398773 -0.69757368  0.98159459  0.20338763
      0.20065954 -0.69480635  0.13080555 -1.06135789 -1.22094525  0.69077871
     -0.30810628 -0.52970262 -0.31741667 -1.16792027  1.37244515  0.61465946
     -1.1460772   2.19535846 -0.20284538  2.12091923  0.89353297 -1.35722971
     -1.5051149  -0.80609311  1.18105177 -0.12682896  0.82333767  0.04509602
      0.06783037 -0.60741282  0.8498689   1.6599338   2.874473    0.33532896
      0.01054717  0.36582558 -0.13257471  1.42387453  1.41771748  0.11318657
      2.12942692 -0.95296388  0.10342228 -0.94628801 -0.52400378 -0.69972257
      0.72114322  0.74139415  0.1687628   1.13880284 -0.12421378 -1.28357734
      1.43410154  1.85929426  0.11341069 -2.48328536  0.92388592  1.79299277
     -0.26542743  2.11709184  0.8314659  -0.95874304  0.4954538  -0.63948485
      0.75919633 -0.03267602  0.88070049 -1.03339834 -0.33738523 -3.15195594
     -0.31105151  2.05227031 -0.56916931 -0.38589659 -1.64912237  0.85012268
      1.20180085  1.10201172 -0.71239728 -0.42834472 -1.70136647  0.33490597
     -0.18967316 -0.47847081  1.97443397  1.71578114  0.67423119  0.58552921
     -0.29979792  1.9508347  -0.99086507  0.24595061  0.74600231 -0.55018364
      1.33432177 -1.1724179   0.29318641  1.37236422 -0.95922365  1.31899361
      0.41669604  0.41343181  0.66168775 -0.21398123  1.3082876  -0.52975884
     -1.07992038 -0.20551144 -0.64061432 -2.25108391  0.85617359 -1.02019677
     -0.97963324  1.06831638  0.34961261  0.84547013  0.37423455  0.16848085
     -0.68913167 -0.48603498  0.96604547  0.6146656   1.72568021 -0.94330816
     -0.02653166  0.02052196 -0.81426363 -0.05290933  0.55588148 -1.39851676
     -0.25642949  0.70735876 -0.77964564 -0.31581728  0.0376585   0.77123715
     -0.02713014  1.5500902   0.65148851 -0.41103639  0.95878757  1.84761562
     -2.11786875 -0.26338872 -1.89760246  0.57076492  0.32464027 -0.97509212
      0.21428294 -0.5739228  -1.83287034  1.52480223  0.14553528 -0.3575835
     -1.24086438 -0.56614126  0.13386323 -1.15041525 -0.73854068 -1.39167217
     -0.97008237 -0.80591976  1.01405565 -1.23211047 -3.17617408  1.31125598
     -0.13866885  0.84100243 -1.14492459 -0.43145417  1.05945389  0.0643646
     -1.53507933 -0.46694015  0.03829063 -0.91603477  0.26973451 -0.12257516
      0.30353128  0.39906484  0.82848788 -1.313071   -0.02530597  1.26732441
      1.1174983   1.52851422  0.31860151  0.60278471  2.32634849 -0.46205015
      1.36678154  0.41680488 -1.13394208 -0.58604189 -0.98276248  1.62562378
     -2.15283315  0.4048864   1.57254583  1.4545733  -1.42731632  0.46287376
      0.20445798 -0.03945198  0.9362183   1.12756403  0.72102925 -1.3318554
      0.62983905 -0.66724327  1.09735032  2.06137616 -1.26424714  0.26379846
     -0.72397096 -0.15803464  0.21673603  0.24125297  0.33194081  0.67464973
      0.15254128  0.3156762  -1.34500945  0.3029607  -1.64165855 -0.80243868
     -0.69666666  0.79116323  0.03638359 -0.11978585  0.68368397  0.42333393
     -1.50145014  0.49076853  0.01907765  0.62265014 -0.40209612 -0.87024692
     -0.07345106  0.12489057 -1.97922203  0.45106028 -0.59412853  0.96732548
      1.91997731  1.3151402  -0.24933197  0.15311825 -0.82445586  1.12829721
     -0.50518069  0.18233572 -1.66309059  0.84491713  0.06574912 -0.29721456
     -0.02037016 -0.2253101  -0.41031557 -1.80683128  0.76653596  0.97948748
     -1.52361813 -0.00918903 -0.07262173 -0.66666077 -0.25963134  0.59786018
     -1.22088862 -1.18089983 -0.44885291 -1.87011707 -0.02244876  0.47966212
      1.03698972  0.39244511 -0.24917861 -0.48105433  1.71252493 -0.94847318
      1.20992768  0.69044    -0.73963222 -0.81751953  1.11988568  0.02297673
      0.61366039 -0.97803005 -1.32522819 -0.30411026  0.45820609 -1.23105057
     -0.3773053  -0.31012375  0.61724965 -0.96910731  0.30384887 -0.44206275
      0.28603583 -1.71153014  0.25110107  0.80831177  0.45157119  0.23818267
     -0.43419181  0.38208766  0.25226856  0.29279375 -0.91282571  1.54931273
     -0.95300864 -0.39791158 -0.98269445  0.22400573 -0.2075856   0.98569658
      0.17050092  1.25774479  2.13084109  0.27512145 -0.99857309  0.51744626
     -0.01752324 -0.47608827  0.21686025 -0.84743564 -1.34739406  1.29915447
      0.80066061 -1.02737035  0.43552495  1.1076967  -0.48287626 -1.31628134
     -0.73297899 -1.75341929 -1.30801162  0.50959372 -0.20265758 -0.09179636
     -1.32806358  1.99012965 -0.15991076  0.61750792 -0.01393644  0.49421556
     -0.93234892  0.51937671  1.25508367  0.0446727  -1.02305984 -1.52551587
      0.06786317 -0.06818574 -1.36369323 -1.11181814 -0.11493041  2.07884059
     -0.927402    0.01141321 -0.06601619 -0.39411336 -0.58514612 -1.02525645
     -0.71510225  0.08803066 -1.6427113   0.13233499  1.14494632  0.69338816
      0.91578524  1.47128653 -0.3839551  -0.08293155 -0.82053835 -1.1091577
     -0.57923978 -0.13888542  0.4935757   0.14916727 -1.11372791  0.35265851
     -0.0591301   0.43671683 -0.76798692  0.68450448 -0.27688679 -0.72916219
     -0.41668438  1.77989324  0.94090852  0.69928063  2.58275936 -0.76837552
     -1.15882868  1.79400768 -1.94157017 -0.30303337 -1.53875985  1.20514896
     -1.61327999 -0.62046659 -0.490873   -0.41723809  0.52166989  0.86789576
      0.08700661 -1.50213458  1.14878337 -1.65004237 -0.25984414  2.45338161
     -3.12313142 -0.08109741  2.54829361  1.095142   -1.35477419  2.54942565
     -0.60365551  0.62914819  1.00749914 -1.33933405 -0.74556816  0.04300631
      0.39354159 -0.8843848   1.17956517  0.66938972 -1.41131576  0.97673902
     -0.16222805 -1.70958639 -0.64378132  0.97205516  0.45123311  0.07340338
     -0.72536706  0.84078852 -1.39262477 -1.84081648 -1.27785203  1.23466935
     -0.21182057 -0.50639619 -1.46318492  0.22030464 -0.51276646 -0.7554345
      0.95389911  1.19254914  2.21375578 -0.58252591 -0.37600981  0.42099504
     -0.70523547  1.35326633  1.11656495 -2.54336622  1.42710755  1.738883
      0.46875664  1.61987027 -0.97077041  0.05475133  0.80971298 -0.47668829
      0.8447907   0.12647021 -1.75776975  0.35169219 -1.80811865 -0.54833596
      0.47908797 -0.19977184 -1.35086292  0.68717421 -0.62140989 -0.6940378
     -1.49653327  0.12427236  1.2373579  -1.6161625   0.23203058  0.77775485
      0.31543858  0.92473939 -1.55995378  0.68714132 -1.64171232 -1.14832905
     -0.68721534 -1.25953993  1.09812604 -0.44475096  0.54008256  0.53677981
      1.69184975 -0.74323608  0.81808547 -1.53947966  1.86575196 -0.25274379
     -0.52323099  1.85663114  1.27417982 -0.64545794  1.77908874 -0.45435003
     -0.87661752 -1.59341955 -0.42918153 -1.0052053  -0.53194006  0.11711482
     -0.48639047  1.35312449  1.93851268 -0.01961836  0.28360396 -0.08312843
      1.09641925 -0.91202655  1.17637459 -0.52222946  0.74987669  0.43052462
     -2.19699484 -0.91649837  0.08171196  1.23248131  0.62880982 -0.18227207
     -0.23493803  0.6083605  -1.10094794  0.20229445  1.17236107 -0.58729544
      0.77734479  0.01781621 -0.89415046 -1.44581643  0.61662392  0.17334784
     -0.03555381 -0.5839866   0.79468432 -0.91777032  0.30982241 -0.04052059
      0.2478152  -1.49328519  0.2887996   1.01430656  1.1240308   0.90620345
      0.04628958 -0.52448352  0.09566368  0.25955799 -0.10007658 -1.01888621
     -0.0369402   1.70970998 -1.00582041  0.90015439  1.09761838  1.26592248
      1.24871045 -0.72802586  1.04629489 -0.9899524   1.33444826 -1.0748812
      0.25284535 -0.33716871  0.35650026  0.50085311  0.82766506 -0.92278585
      0.13145819  0.36772851  0.47304718  0.4079768   0.2586168  -1.40576595
      0.64411305 -0.31455724  0.88462882 -0.87966947  0.37262338 -1.36824762
      0.84713205 -0.75419448  1.22260115  1.39726965]
    0.0
    Final model:
    [  6.04001334e-02   9.60248722e-02   5.43810972e-02   9.44754148e-02
      -3.42695982e-02   4.75053617e-02   1.21334123e-01  -7.97097590e-02
       1.01688015e-01   7.00784288e-02   8.18538729e-02   5.24220545e-02
      -9.81842159e-02  -3.54241602e-03   1.43694571e-01   8.09641880e-03
      -1.54832446e-01   4.88004882e-02  -1.04383573e-02  -2.01025766e-01
       4.56619080e-02   5.48996549e-02  -5.67554933e-02   1.14865842e-01
       4.99582407e-02  -3.08287890e-02  -3.13050055e-03  -8.89690544e-02
      -9.00270280e-02   1.98789538e-01   1.32462260e-01   1.59973053e-01
      -1.20793733e-01   7.04522827e-02   1.03111283e-01  -5.39406591e-02
      -5.87535948e-02  -1.27934002e-01  -9.46341216e-02   1.11146401e-01
       4.73844646e-02   7.63129671e-02   1.51672198e-01   4.76205399e-02
       5.81481382e-03   5.62740914e-02  -1.49432410e-01   5.63810825e-02
      -2.56861524e-02   3.52879019e-02  -1.59096076e-01   1.62350916e-01
      -3.08293250e-02  -6.04976104e-02  -2.40604773e-02  -3.70552255e-02
      -7.44932773e-02  -7.87314182e-02  -5.28903603e-02  -4.99932247e-02
      -1.09438965e-01  -1.51439240e-01  -3.14730295e-02  -9.14232437e-02
      -4.85408247e-02  -1.85807493e-01  -3.48467263e-02   5.44626580e-02
      -1.21812194e-02   1.75833249e-02  -4.55336765e-02   4.44141142e-02
       1.15183309e-01  -4.66236379e-02   1.07664162e-02  -6.27553586e-02
       1.37506256e-01   8.70275343e-03  -6.03157319e-02  -4.83425258e-02
       5.16912269e-02   6.11927821e-02  -2.45319927e-02   2.23852228e-02
       5.14961510e-02   2.87774680e-02  -4.33037352e-02   1.85779466e-02
      -1.26033539e-01   4.16442158e-02   4.96403903e-02  -3.29944412e-02
       1.49879223e-01   1.02093581e-01   1.85321348e-01  -3.86950377e-03
       6.20593034e-02   1.25403499e-01  -1.03070906e-01  -2.27364773e-01
      -1.79672095e-02   7.50062370e-02  -1.07775661e-01   1.48888708e-01
       8.98467170e-02   2.02650274e-02   1.19004996e-01   3.95743960e-04
      -5.24859630e-02  -1.64369067e-01   1.96886814e-02   9.61943723e-02
       4.42777175e-02   3.99540891e-02   1.06601584e-01   1.29582258e-01
       4.45516111e-02   3.27874427e-02  -4.46925011e-02  -8.84215468e-02
       1.58659619e-01  -3.53808946e-02  -1.16269330e-01  -1.15950680e-01
      -1.91402323e-02  -1.45184193e-01   4.39685962e-02  -1.01695270e-02
      -2.15661295e-01   5.19104629e-02  -1.54584227e-02   1.30641857e-02
       1.56234843e-01   1.50104467e-01  -2.14796438e-02   2.87770985e-02
       1.54345381e-01  -1.59408711e-01  -5.26407684e-02  -3.99633079e-02
       7.84474055e-02  -1.03297717e-01  -1.05550585e-01   5.78895439e-02
       6.84067462e-02   7.12318406e-02  -4.13273921e-03  -1.59800991e-01
      -1.26726187e-01  -3.00434903e-02   4.90060697e-02   8.01722631e-03
       1.39568245e-01  -1.62733529e-01   1.25195124e-01  -6.02192066e-02
       1.02787858e-02  -2.06276084e-02  -1.96570786e-02  -6.87970075e-02
       2.70375212e-02   6.48660209e-02   1.05909312e-01  -6.25086420e-02
      -1.11431500e-01   8.60448411e-02   7.57578188e-02   6.56370811e-02
       1.13284213e-01  -2.03365322e-01  -9.05371615e-02   1.20718980e-01
      -3.94678608e-03   9.76047751e-03   2.79576031e-02  -6.15885362e-04
       1.46013689e-01  -6.37629578e-02   3.95680117e-02  -8.42141574e-02
      -1.42768523e-01   1.30699955e-01   2.65959933e-02  -4.52981032e-02
       1.04690435e-02  -5.01510390e-02  -3.81875449e-02  -6.15247528e-02
       8.41315337e-03   1.53333617e-02  -4.04568885e-05   2.83224467e-01
      -1.26953657e-01  -9.61975036e-02  -6.53801771e-02   2.42370117e-02
      -8.40981084e-04   8.84287816e-02  -1.11435134e-01  -2.02141417e-02
      -6.32711401e-03  -9.58799347e-02   3.95701993e-02  -5.90945854e-02
       1.87656045e-03   1.14712515e-01   5.84857734e-03  -2.89582575e-02
      -8.88798398e-02   1.00139126e-01   8.11125833e-02   1.31819238e-01
       4.26876459e-02  -1.06882270e-02   1.87385402e-01   1.20612441e-01
       2.93314513e-02   2.48585785e-02  -3.68111497e-02  -1.24852173e-02
      -1.14593373e-01  -5.81851446e-02  -3.58542383e-02  -3.68216189e-02
       2.14397990e-02  -1.09756290e-01   1.26506350e-02   5.07801923e-02
       6.37133454e-02  -1.23716491e-01  -4.65442378e-02  -7.06963853e-04
      -6.13445917e-02  -1.27641055e-02  -3.67547065e-02   3.76676953e-02
      -1.18834554e-01  -5.09305329e-03   2.67984083e-02  -3.18146402e-02
      -1.94810452e-02   2.63646978e-02   8.25616162e-02   6.02029884e-02
       9.43886002e-02  -1.13101686e-01  -2.85726262e-01  -1.43205205e-01
      -1.14023899e-01  -5.02915575e-02   4.64056039e-02   1.06393709e-01
       1.95350594e-01   3.50156370e-02  -6.40208032e-02   7.87826202e-02
       6.21872290e-02   5.54057169e-02   1.35708778e-01   2.65770955e-02
      -1.07218161e-01  -1.72411200e-02   5.42353952e-02   1.81427298e-01
       1.88521658e-01  -2.12768504e-01  -1.20668989e-01   8.62554679e-02
       7.29513492e-02   3.63255964e-03  -6.73243664e-02  -2.15290834e-01
      -6.60531361e-02  -9.03994458e-04   5.91615375e-02   2.77812202e-02
       5.60816302e-02   1.27503963e-01   4.38042426e-02   4.93819324e-02
       1.36700843e-01  -2.38526104e-02   4.15859943e-03  -8.49570581e-03
      -7.03770802e-02  -4.52693173e-02   7.53310852e-03  -1.00311522e-01
       1.38704671e-01  -1.66724763e-01  -1.42077534e-01   7.05537381e-02
       8.50878917e-02  -1.05456937e-01  -1.93995750e-02   1.49720518e-01
       3.52674600e-02  -1.65531429e-01  -8.23813314e-02   4.95412735e-02
      -3.89965227e-02   4.05577380e-02   1.22729764e-01   1.05285491e-01
       3.75089624e-03   1.06431612e-01  -2.34494459e-02   1.05030657e-01
       6.31999373e-02  -6.38965908e-03   6.14076317e-02   5.93457054e-03
       6.66967507e-02   3.63408765e-02   1.26488149e-02   2.59008763e-02
       6.99926188e-02  -4.35998655e-02  -2.34503915e-01  -5.37715092e-02
       1.09672966e-01  -1.15602981e-01  -2.45870146e-02   9.09252942e-04
       6.41185780e-03  -4.98813760e-02   1.38840867e-02   1.73630830e-02
       1.39810863e-01   5.11800908e-02   4.49930243e-02  -5.88715426e-02
      -7.85274599e-02   5.07581719e-02   6.39309057e-02   5.44339723e-02
       2.96829648e-02   4.76558913e-02  -1.18330054e-01  -4.13840924e-02
       1.68363982e-01   2.36316921e-02  -5.28146031e-02   1.51539835e-02
      -1.02076723e-01  -6.49839397e-02  -9.17730992e-02   2.30978096e-01
      -2.30286284e-01  -1.01827437e-01   8.40047921e-02  -1.70289776e-02
      -4.15368563e-02  -1.64016964e-02   3.24830056e-02  -6.91253918e-02
       1.38117116e-03  -1.03547710e-01  -7.68499047e-02  -1.23680635e-02
       1.41545751e-01  -1.04422268e-01   1.46009247e-01   9.12898372e-02
      -2.91279823e-02   1.74243738e-01   6.64518645e-02  -1.83489958e-01
      -3.16025570e-02  -3.48481241e-03  -4.97881353e-02  -1.90614105e-01
       7.12617593e-02   3.64966049e-02  -1.16666538e-01  -2.33980217e-02
       5.06770699e-02   7.54664452e-02  -2.75343503e-02   1.00551076e-01
      -6.35019353e-02   6.29023211e-03  -1.33383833e-02   1.49923888e-01
       1.79575177e-02   4.42060751e-02  -1.02950033e-01   6.54840170e-02
      -8.04196879e-02   8.45980484e-02   1.41429851e-01  -2.64998543e-02
      -9.93514735e-02   2.50342346e-03   8.12652666e-02  -7.31710524e-03
       1.33571082e-01  -6.02450847e-03  -1.45802213e-01  -2.86241088e-02
       2.98961276e-02   6.60617780e-02   6.19155400e-02  -1.28402481e-01
       7.44770103e-02   3.59530802e-02  -2.12352694e-02   1.27264203e-02
       1.66584325e-01   8.33214862e-02   2.10052915e-02   9.79132046e-02
      -2.10099645e-01  -4.70488118e-02   6.71292896e-02  -1.07232641e-01
       5.81596375e-02  -1.31357454e-02  -1.55360295e-02  -9.75279208e-02
       7.81805336e-02  -5.81683292e-02   1.11443459e-01   2.06146901e-02
      -3.72571245e-02   1.89455429e-02   9.52351267e-02   1.49818138e-01
      -2.47806466e-02   1.97619122e-01  -6.37171631e-02   7.23098764e-02
       1.81316947e-01   8.09515362e-02   1.25012158e-01   2.35886608e-02
       2.08855993e-02  -1.44991631e-01  -5.64429559e-02   2.95047942e-02
      -1.29521558e-02  -1.10420453e-01   1.82959283e-01   7.71339671e-03
       4.98049138e-02   1.39306408e-03  -2.32534217e-02   1.28025334e-01
      -6.71011833e-03   5.22272124e-02  -2.08917478e-02  -1.41020393e-02
       1.50295866e-03  -9.68240246e-05   2.27860746e-03  -4.32140751e-02
      -3.15728963e-02   1.75079658e-01   2.68950232e-02   1.23783358e-01
      -5.21410243e-02   1.05610607e-01  -1.21466739e-01  -1.36250946e-01
      -2.69985121e-02   1.05868020e-01  -2.22171232e-01   2.29225597e-02
       8.12323569e-02   1.36502695e-01  -4.64713772e-02   1.16747960e-01
      -8.00825916e-02  -2.47740982e-03   1.06054223e-01  -4.55036559e-02
      -2.09656178e-01  -5.97802058e-02  -4.62647577e-02   6.18694466e-02
       3.42790797e-02  -9.50590731e-02  -1.43891090e-02  -6.42231405e-02
       4.41077299e-02  -1.21894134e-01  -3.67214105e-02   1.26247173e-01
      -1.38667372e-02   7.82119035e-02   9.90494337e-02  -1.16001152e-01
       1.15306293e-01   7.01243484e-03  -5.78210730e-02   1.12887046e-01
       1.45803198e-01   1.56292379e-02  -2.29789436e-02   8.49468743e-02
      -1.31927457e-01  -3.97056725e-02  -2.04422611e-02  -3.12821001e-02
      -1.67411968e-02   4.30593969e-02   5.86472217e-02  -1.73821159e-01
       1.94449060e-02   4.13733103e-02  -8.67562268e-02  -8.30766972e-03
       5.92665453e-02   2.58883518e-02  -1.00939566e-01   3.89288430e-02
       5.54489723e-02  -4.47730473e-02  -3.64595350e-03   1.15583955e-02
      -1.25891908e-01  -6.28197242e-03   1.83419592e-02   7.75742263e-02
       1.23197849e-01  -2.75851464e-02  -2.82815489e-02   6.87257647e-02
      -1.21410564e-01  -3.84227612e-02  -8.53240094e-03  -1.17859276e-01
       2.08576068e-02   1.79017309e-01   6.85133162e-02   6.71264410e-02
       1.62353769e-02   2.35210286e-01   8.96783966e-02  -7.75751994e-02
      -4.65040831e-02  -1.21766649e-01  -3.87794550e-02  -4.06648423e-02
      -9.33037319e-02  -6.05247231e-02   1.33345465e-01   1.07143626e-02
       1.56358885e-01  -1.07432619e-01  -7.32225471e-02   1.09655540e-01
       9.42642792e-02   3.79242488e-02  -1.14741914e-01  -1.44669740e-01
      -5.18051065e-02   3.74044393e-03  -1.81055713e-02  -5.56404904e-02
       4.96744283e-03   4.29994484e-02  -2.42472422e-03   6.58726682e-02
       1.98057030e-01   1.57039964e-01  -1.14109178e-01  -2.01782603e-01
       9.88367636e-02   1.86723713e-01  -6.14865617e-02  -4.43566711e-02
      -1.56923577e-01   1.48903963e-02   2.33294632e-02   3.42058704e-02
      -1.10749269e-02   6.51381709e-02  -6.80574676e-02   2.17986279e-01
      -9.85376032e-02   5.39891339e-02   3.41407289e-02   5.41615498e-02
       1.16502158e-02   1.26122501e-01   2.92532089e-02  -8.74009759e-02
       1.27597942e-02   4.84057845e-02   7.42343566e-02  -1.29861983e-03
       5.79949203e-02   2.44919010e-02  -2.00340219e-03   1.01642165e-01
       6.97362739e-02  -2.23000329e-02   1.37445761e-01  -1.54415714e-02
       3.53729984e-02  -6.48817513e-03  -3.53833244e-02  -3.58076372e-02
       8.04865828e-02   2.06595949e-02   2.32325071e-01  -3.26278972e-02
       4.56484413e-02   4.21896492e-01  -6.79063602e-02   4.72288647e-03
      -4.10572692e-02  -2.96388191e-02  -2.33278070e-02  -8.00289324e-02
      -4.28578267e-02   2.67919153e-02   9.13607976e-02   8.47168793e-02
       3.25184138e-02   1.80763086e-01   3.99788712e-02   1.41933814e-01
       6.09682347e-02   3.57355381e-02   5.05444011e-02  -5.98200707e-02
       3.32554526e-02  -1.92815673e-01   2.93640369e-02   1.90090289e-02
       1.33997019e-01   2.28136462e-01   1.83884375e-02  -7.59656250e-03
       9.34507313e-02   2.26615866e-03   9.23422276e-02   5.69229082e-03
      -1.12260724e-01  -1.35482214e-01  -1.05577133e-02  -6.03425871e-02
      -2.21063326e-01   2.62668393e-03   1.60977235e-02   7.73699060e-02
      -1.18112780e-01  -1.36556982e-01   1.01640417e-01   5.62991425e-02
       1.56663553e-01   7.43660470e-02  -1.28647064e-01  -5.69425834e-03
       1.14098042e-01   1.75496745e-02  -2.27572047e-01  -5.15637287e-03
       9.60288308e-03  -1.17127742e-01   6.47249212e-02   1.54728163e-01
       4.36995108e-02  -1.05602228e-01  -6.73333134e-02   2.87523111e-02
       6.95272611e-02   1.41985072e-02   1.20908465e-01  -2.99408995e-03
      -4.36028604e-02  -3.95802572e-02   1.92543075e-02   2.29225964e-01
       1.17774477e-02   7.14082097e-02  -2.91025398e-02   3.25576633e-02
       1.62949111e-01  -8.20001988e-03   1.02807716e-01   1.26411616e-02
       1.19365728e-01  -3.63814932e-02  -6.02795675e-02  -3.46733711e-02
       7.83355047e-02   1.28214749e-01  -5.12648981e-02   8.96120440e-02
       7.58133003e-02  -8.12066455e-02   1.12575677e-01  -1.31814194e-02
      -1.38950216e-01  -5.97314663e-02  -2.33643707e-01   2.03906359e-01
       7.58091084e-02   4.40637860e-02  -4.56527939e-02  -5.14027340e-03
      -2.96364275e-03  -1.95993178e-02  -1.60348814e-01  -5.68991701e-02
      -6.38171475e-02   1.32543377e-02  -5.67604429e-02   1.29579348e-01
      -4.19184347e-02  -1.93038772e-02  -2.69406651e-01   1.02206478e-01
      -6.94640892e-03  -4.72919096e-02   1.25712483e-01  -3.98364461e-02
       2.03816465e-01   9.12776722e-02   2.90981871e-02   5.64469324e-02
       4.91444430e-02  -7.94617973e-02   1.99949850e-02   5.49304770e-02
       1.69666439e-02  -6.03169202e-02  -3.80268734e-02  -4.05717770e-02
      -2.93996509e-02   1.24324222e-01   1.91321661e-02  -1.34276924e-01
      -1.60855690e-01  -7.86241012e-02   7.15606826e-02   3.59137597e-02
      -1.16174035e-01  -1.08171441e-02  -7.42028362e-02  -9.01611583e-02
      -3.62563761e-03   8.39547970e-02   1.05810124e-02   1.18532732e-01
      -5.94030166e-02   1.36395518e-01   1.03004388e-01  -4.17299278e-02
       7.52368682e-02   9.49700167e-03  -2.25218052e-01  -2.96282264e-02
       4.17611348e-02   7.62843648e-02   4.04114877e-02  -1.73441292e-02
      -1.06749690e-01   9.53167609e-02   9.62529451e-02  -2.63529406e-03
      -1.06825027e-01  -3.03511283e-01  -7.47227044e-02   2.92586848e-02
       6.80797583e-02   2.12227245e-01   2.25542728e-02   7.84695084e-02
      -1.09803788e-02  -7.30554186e-02   1.57031143e-01   4.27847118e-02
      -9.52403027e-02  -1.12643159e-02  -1.98788556e-02  -2.82465016e-02
      -1.12814832e-01  -7.77743396e-02  -8.64426740e-02   5.09154425e-02]
    -0.380923129054
    target values for D:
    [1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0
     0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1
     0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0
     1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1
     1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0
     1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1
     0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0
     1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1
     1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1
     0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1
     1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1]
    prediction on D:
    [1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0
     0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1
     0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0
     1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1
     1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0
     1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1
     0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0
     1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1
     1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1
     0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1
     1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1]


**Optimizations:**

Where are those optimization applied?

* ``log(1+exp(x))``

* ``1 / (1 + T.exp(var))`` (sigmoid)

* ``log(1-sigmoid(var))`` (softplus, stabilisation)

* GEMV (matrix-vector multiply from BLAS)

* Loop fusion


.. testcode::

  p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b))
  # 1 / (1 + T.exp(var)) -> sigmoid(var)
  xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1)
  # Log(1-sigmoid(var)) -> -sigmoid(var)
  prediction = p_1 > 0.5
  cost = xent.mean() + 0.01*(w**2).sum()
  gw,gb = T.grad(cost, [w,b])

  train = theano.function(
            inputs=[x,y],
            outputs=[prediction, xent],
            # w-0.1*gw: GEMV with the dot in the grad
            updates={w:w-0.1*gw, b:b-0.1*gb})


Theano flags
------------

Theano can be configured with flags. They can be defined in two ways

* With an environment variable: ``THEANO_FLAGS="mode=ProfileMode,ProfileMode.profile_memory=True"``

* With a configuration file that defaults to ``~/.theanorc``


Exercise 2
-----------

.. testcode::
    
    import numpy
    import theano
    import theano.tensor as T
    rng = numpy.random

    N = 400
    feats = 784
    D = (rng.randn(N, feats).astype(theano.config.floatX),
    rng.randint(size=N,low=0, high=2).astype(theano.config.floatX))
    training_steps = 10000

    # Declare Theano symbolic variables
    x = T.matrix("x")
    y = T.vector("y")
    w = theano.shared(rng.randn(feats).astype(theano.config.floatX), name="w")
    b = theano.shared(numpy.asarray(0., dtype=theano.config.floatX), name="b")
    x.tag.test_value = D[0]
    y.tag.test_value = D[1]
    #print "Initial model:"
    #print w.get_value(), b.get_value()


    # Construct Theano expression graph
    p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b)) # Probability of having a one
    prediction = p_1 > 0.5 # The prediction that is done: 0 or 1
    xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) # Cross-entropy
    cost = xent.mean() + 0.01*(w**2).sum() # The cost to optimize
    gw,gb = T.grad(cost, [w,b])

    # Compile expressions to functions
    train = theano.function(
                inputs=[x,y],
                outputs=[prediction, xent],
                updates={w:w-0.01*gw, b:b-0.01*gb},
                name = "train")
    predict = theano.function(inputs=[x], outputs=prediction,
                name = "predict")

    if any( [x.op.__class__.__name__=='Gemv' for x in
    train.maker.fgraph.toposort()]):
        print 'Used the cpu'
    elif any( [x.op.__class__.__name__=='GpuGemm' for x in
    train.maker.fgraph.toposort()]):
        print 'Used the gpu'
    else:
        print 'ERROR, not able to tell if theano used the cpu or the gpu'
        print train.maker.fgraph.toposort()



    for i in range(training_steps):
        pred, err = train(D[0], D[1])
    #print "Final model:"
    #print w.get_value(), b.get_value()

    print "target values for D"
    print D[1]

    print "prediction on D"
    print predict(D[0])

    # Print the graph used in the slides
    theano.printing.pydotprint(predict,
                               outfile="pics/logreg_pydotprint_predic.png",
                               var_with_name_simple=True)
    theano.printing.pydotprint_variables(prediction,
                               outfile="pics/logreg_pydotprint_prediction.png",
                               var_with_name_simple=True)
    theano.printing.pydotprint(train,
                               outfile="pics/logreg_pydotprint_train.png",
                               var_with_name_simple=True)

Modify and execute the example to run on CPU with floatX=float32

* You will need to use: ``theano.config.floatX`` and ``ndarray.astype("str")``

GPU
---

* Only 32 bit floats are supported (being worked on)
* Only 1 GPU per process
* Use the Theano flag ``device=gpu`` to tell to use the GPU device
  
 * Use ``device=gpu{0, 1, ...}`` to specify which GPU if you have more than one
 * Shared variables with float32 dtype are by default moved to the GPU memory space

* Use the Theano flag ``floatX=float32``

 * Be sure to use ``floatX`` (``theano.config.floatX``) in your code
 * Cast inputs before putting them into a shared variable
 * Cast "problem": int32 with float32 to float64
    
  * A new casting mechanism is being developed
  * Insert manual cast in your code or use [u]int{8,16}
  * Insert manual cast around the mean operator (which involves a division by the length, which is an int64!)



Exercise 3
-----------

* Modify and execute the example of `Exercise 2`_ to run with floatX=float32 on GPU

* Time with: ``time python file.py``

Symbolic variables
------------------

* # Dimensions
    
 * T.scalar, T.vector, T.matrix, T.tensor3, T.tensor4

* Dtype

 * T.[fdczbwil]vector (float32, float64, complex64, complex128, int8, int16, int32, int64)

 * T.vector to floatX dtype

 * floatX: configurable dtype that can be float32 or float64.

* Custom variable

 * All are shortcuts to: ``T.tensor(dtype, broadcastable=[False]*nd)``

 * Other dtype: uint[8,16,32,64], floatX

Creating symbolic variables: Broadcastability

* Remember what I said about broadcasting?

* How to add a row to all rows of a matrix?

* How to add a column to all columns of a matrix?


Details regarding symbolic broadcasting...
  
* Broadcastability must be specified when creating the variable

* The only shorcut with broadcastable dimensions are: **T.row** and **T.col**

* For all others: ``T.tensor(dtype, broadcastable=([False or True])*nd)``


Differentiation details
-----------------------

>>> gw,gb = T.grad(cost, [w,b])

* T.grad works symbolically: takes and returns a Theano variable

* T.grad can be compared to a macro: it can be applied multiple times

* T.grad takes scalar costs only

* Simple recipe allows to compute efficiently vector x Jacobian and vector x Hessian

* We are working on the missing optimizations to be able to compute efficently the full Jacobian and Hessian and Jacobian x vector


.. _cifar2011_benchmark:

Benchmarks
----------


**Multi-Layer Perceptron**:

60x784 matrix times 784x500 matrix, tanh, times 500x10 matrix, elemwise, then all in reverse for backpropagation

.. image:: ../hpcs2011_tutorial/pics/mlp.png

**Convolutional Network**: 

256x256 images convolved with 6 7x7 filters,
downsampled to 6x50x50, tanh, convolution with 16 6x7x7 filter, elementwise
tanh, matrix multiply, softmax elementwise, then in reverse

.. image:: ../hpcs2011_tutorial/pics/conv.png

**Elemwise**

* All on CPU
* Solid blue: Theano
* Dashed Red: numexpr (without MKL)

.. image:: ../hpcs2011_tutorial/pics/multiple_graph.png
