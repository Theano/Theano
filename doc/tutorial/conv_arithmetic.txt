.. _conv_arithmetic:

======================
Convolution arithmetic
======================

The relationship between a convolution operation's input shape, kernel size,
stride, padding and its output shape can be confusing at times.

This tutorial aims at providing an intuitive understanding of how the different
convolution parameters influence the shape of the output it produces.

Refresher: discrete convolution
===============================

.. todo::

    WRITEME. Talk briefly about N-D convolutions.

Here are the factors determining the shape of a convolution's output size along
each given axis (height, width, ...):

* **Input size**: length of the input along the given axis.
* **Kernel size**: length of the kernel along the given axis.
* **Stride**: distance between two consecutive positions of the kernel along
  the given axis.
* **(Zero-) padding size**: number of zero values concatenated at the beginning
  and at the end of the axis.

.. note::
    For the remainder of this tutorial, we will concentrate on 2D convolutions
    mapping one input feature map to one output feature map. The relationships
    presented here also hold in the general case, i.e. an N-dimensional
    convolution with arbitrary input shape, kernel shape, zero padding and
    strides.

    Since height and width are treated intependently in computing a
    convolution's output shape, we will also restrict ourselves to square input,
    kernel, stride and padding shapes.

Simplest case
=============

Let's start with the simplest case, where a kernel just slides across the input:

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p = 0` (``border_mode='valid'`` in Theano).

Here is what the convolution looks like for :math:`i = 4` and :math:`k = 3`:

.. figure:: conv_arithmetic_figures/no_padding_no_strides.gif

One way of defining the output size is by the number of possible placements of
the kernel on the input. Let's consider the x-axis: the left side of the kernel
starts at the left side of the input, and we slide the kernel until its right
side touches the right side of the input. The size of the output will be equal
to the number of steps we need to make, plus one, because we need to account for
the initial position of the kernel. The same applies for the y-axis.

.. todo::

    Add figure to illustrate the previous paragraph.

From this, we conclude that the output size :math:`o` of the convolution is

.. math::

    o = (i - k) + 1.

Zero padding
============

If we factor in zero padding, i.e.

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p`,

all we need to do is to apply the previous rule on the augmented input. Since
padding with :math:`p` zeros adds :math:`2p` to the input size (:math:`p` on
each side), the output size :math:`o` of the convolution becomes

.. math::

    o = (i - k) + 2p + 1.

Here is an example for :math:`i = 4`, :math:`k = 4` and :math:`p = 2`:

.. figure:: conv_arithmetic_figures/arbitrary_padding_no_strides.gif

Zero padding: special cases
===========================

There are two special cases of zero padding to take note of, because they're
quite useful in practice.

Same
----

If :math:`k` is odd (:math:`k = 2n + 1`) and
:math:`p = \lfloor k / 2 \rfloor = n`, we have that

.. math::

    \begin{split}
    o &= i + 2 \lfloor k / 2 \rfloor - (k - 1) \\
      &= i + 2n - 2n \\
      &= i.
    \end{split}

In other words, the output size is equal to the input size. This is sometimes
referred to as ``'same'`` padding. Here is an example for :math:`i = 3`,
:math:`k = 3` and :math:`p = 1`:

.. figure:: conv_arithmetic_figures/same_padding_no_strides.gif

Full
----

If :math:`p = k - 1`, we have that

.. math::

    \begin{split}
    o &= i + 2(k - 1) - (k - 1) \\
      &= i + (k - 1).
    \end{split}

The input size is *increased* by :math:`k - 1` rather than decreased by
:math:`k - 1`. This is sometimes referred to as ``'full'`` padding. Here is
an example for :math:`i = 3`, :math:`k = 3` and :math:`p = 2`:

.. figure:: conv_arithmetic_figures/full_padding_no_strides.gif

Strides
=======

Up until now, we only considered convolutions where the kernel was translated by
increments of :math:`s = 1`. Let's now examine what happens when :math:`s > 1`.
To keep things simple, let's ignore padding, i.e.

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s`,
* padding size :math:`p = 0`.

Here is what the strided convolution looks like for :math:`i = 5`, :math:`k = 3`
and :math:`s = 2`:

.. figure:: conv_arithmetic_figures/no_padding_strides.gif

Once again, we are counting the number of positions where the kernel can be
placed.  Let's consider the x-axis: the left side of the kernel starts at the
left side of the input, but this time we slide the kernel **by hops of**
:math:`s` until its right side touches the right side of the input. The size of
the output will be equal to the number of steps we need to make, plus one,
because we need to account for the initial position of the kernel. The same
applies for the y-axis.

.. todo::

    Add figure to illustrate the previous paragraph.

From this, we conclude that the output size :math:`o` of the strided convolution
is

.. math::

    o = \left\lfloor \frac{i - k}{s} \right\rfloor + 1.

Combining padding and strides
=============================

The full picture, i.e.

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s`,
* padding size :math:`p`,

is obtained by applying the previous rule on the input augmented by the padding.
In other words,

.. math::

    o = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1.

Here is what it looks like for :math:`i = 5`, :math:`k = 3`, :math:`s = 2` and
:math:`p = 1`:

.. figure:: conv_arithmetic_figures/padding_strides.gif

Transposed convolution: convolution turned on its head
======================================================

Every convolution boils down to an efficient implementation of a matrix
operation. Take for instance the very first convolution presented in this
tutorial. If the input and output were to be unrolled into vectors from left to
right, top to bottom, the matrix representation of the convolution would look
like

.. math::

    \begin{pmatrix}
    w_{0,0} & 0       & 0       & 0       \\
    w_{0,1} & w_{0,0} & 0       & 0       \\
    w_{0,2} & w_{0,1} & 0       & 0       \\
    0       & w_{0,2} & 0       & 0       \\
    w_{1,0} & 0       & w_{0,0} & 0       \\
    w_{1,1} & w_{1,0} & w_{0,1} & w_{0,0} \\
    w_{1,2} & w_{1,1} & w_{0,2} & w_{0,1} \\
    0       & w_{1,2} & 0       & w_{0,2} \\
    w_{2,0} & 0       & w_{1,0} & 0       \\
    w_{2,1} & w_{2,0} & w_{1,1} & w_{1,0} \\
    w_{2,2} & w_{2,1} & w_{1,2} & w_{1,1} \\
    0       & w_{2,2} & 0       & w_{1,2} \\
    0       & 0       & w_{2,0} & 0       \\
    0       & 0       & w_{2,1} & w_{2,0} \\
    0       & 0       & w_{2,2} & w_{2,1} \\
    0       & 0       & 0       & w_{2,2} \\
    \end{pmatrix}^T

where :math:`w_{i,j}` is the element of the kernel located at row :math:`i` and
column :math:`j`. (*Note: the matrix has been transposed for formatting
purposes.*) This linear operation takes a 16-dimensional vector as input and
produces a 4-dimensional vector as output.

What would happen if we were to transpose this matrix and feed the corresponding
linear operation a 4-dimensional vector as input? We would get a 16-dimensional
vector as output. This is what we call a **transposed convolution**.

Transposed convolutions share the same connectivity pattern as their converse
convolution, which makes them suitable for decoders in a convolutional
autoencoder, for instance.

Simplest case, transposed
=========================

If we visualize the effect of applying the transposed matrix above on a
:math:`4 \times 4` input, this is what we get:

.. figure:: conv_arithmetic_figures/no_padding_no_strides_transposed.gif

Note that although equivalent to the transposed matrix, this visualization adds
a lot of zero multiplications in the form of zero padding. This is done for
illustration purposes, but it is inefficient, and software implementations will
normally not perform the useless zero multiplications.

One way to understand the logic behind zero padding is to consider what would
be necessary to produce a transposed convolution with the same connectivity
pattern as its converse convolution.

In particular, the corner pixels of the input only contribute to the corner
pixels of the output: the top left input pixel is only connected to the top left
output pixel, the top right pixel is only connected to the top right output
pixel, and so on. This is achieved by zero padding the input such that the
kernel only touches a corner pixel when it is in the associated corner position.

You can also verify that the connectivity pattern is respected everywhere else
in the input. For instance, when the kernel has been translated by one row and
one column, it covers the whole input. This means that in the converse
convolution, the input pixel located at the first row and first column
contributes to all output pixels.

In general, a convolution described by

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p = 0`

has a transpose described by

* input size :math:`i' = i - (k - 1)`,
* kernel size :math:`k' = k`,
* stride :math:`s' = 1`,
* padding size :math:`p' = k - 1`.

Note that this corresponds to a "fully padded" convolution:

.. math::

    \begin{split}
    o &= i' + 2(k - 1) - (k - 1) \\
      &= i - (k - 1) + (k - 1) \\
      &= i.
    \end{split}

Zero padding, transposed
========================

If the transpose of a non-padded convolution involves adding zero-padding, what
does the transpose of a zero-padded convolution look like? It turns out that it
involves *removing* some padding, i.e. a convolution described by

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p`

has a transpose described by

* input size :math:`i' = i + 2p - (k - 1)`,
* kernel size :math:`k' = k`,
* stride :math:`s' = 1`,
* padding size :math:`p' = k - p - 1`.

The output size is

.. math::

    \begin{split}
    o &= i' + 2(k - p - 1) - (k - 1) \\
      &= i + 2p - (k - 1) - 2p + 2(k - 1) - (k - 1) \\
      &= i.
    \end{split}

Here is what the transpose of a convolution looks like for :math:`i = 4`,
:math:`k = 4` and :math:`p = 2`:

.. figure:: conv_arithmetic_figures/arbitrary_padding_no_strides_transposed.gif

Zero padding, transposed: special cases
=======================================

Same
----

Unsurprisingly, the transpose of a "same padded" convolution is itself a "same
padded" convolution, i.e. a convolution described by

* input size :math:`i`,
* kernel size :math:`k = 2n + 1`,
* stride :math:`s = 1`,
* padding size :math:`p = \lfloor k / 2 \rfloor = n`

has a transpose described by

* input size :math:`i' = i`,
* kernel size :math:`k' = k`,
* stride :math:`s' = 1`,
* padding size :math:`p' = p`.

Here is what the transpose of a convolution looks like for :math:`i = 3`,
:math:`k = 3` and :math:`p = 1`:

.. figure:: conv_arithmetic_figures/same_padding_no_strides_transposed.gif

Full
----

If the transpose of a convolution with no padding is a "fully padded"
convolution, then the transpose of a "fully padded" convolution is... a
convolution with no padding. A convolution described by

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p = k - 1`

has a transpose described by

* input size :math:`i' = i + (k - 1)`,
* kernel size :math:`k' = k`,
* stride :math:`s' = 1`,
* padding size :math:`p' = 0`.

Here is what the transpose of a convolution looks like for :math:`i = 3`,
:math:`k = 3` and :math:`p = 2`:

.. figure:: conv_arithmetic_figures/full_padding_no_strides_transposed.gif

Strides, transposed
===================

If a zero-padded convolution has a transpose with padding removed, then a
convolution with strides greater than 1 has a transpose with strides... smaller
than 1? In some sense, yes: in order to translate the kernel fractionally, we
insert zeros between input pixels.

Once again, doing so is inefficient and real-world implementations avoid useless
multiplications by zero, but conceptually it's how the transpose of a strided
convolution can be thought of. A convolution described by

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s`,
* padding size :math:`p = 0`

has a transpose described by

* input size :math:`i' = \left\lfloor \frac{i - k}{s} \right\rfloor + 1`,
* stretched input size :math:`\tilde{i}' = i' + s (i' - 1)`,
* kernel size :math:`k' = k`,
* stride :math:`s' = 1`,
* padding size :math:`p' = k - 1`

where the input is stretched to size :math:`\tilde{i}'` by adding :math:`s - 1`
zeros between each pixel.

Here is what the transpose of a convolution looks like for :math:`i = 5`,
:math:`k = 3` and :math:`s = 2`:

.. figure:: conv_arithmetic_figures/no_padding_strides_transposed.gif

Combining padding and strides, transposed
=========================================

Zero-padding and non-unit strides are treated independently to transpose a
convolution. This means that in the most general case a convolution described by

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s`,
* padding size :math:`p`

has a transpose described by

* input size :math:`i' = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1`,
* stretched input size :math:`\tilde{i}' = i' + s (i' - 1)`,
* kernel size :math:`k' = k`,
* stride :math:`s' = 1`,
* padding size :math:`p' = k - p - 1`

where the input is stretched to size :math:`\tilde{i}'` by adding :math:`s - 1`
zeros between each pixel.

Here is what the transpose of a convolution looks like for :math:`i = 5`,
:math:`k = 3`, :math:`s = 2` and :math:`p = 1`:

.. figure:: conv_arithmetic_figures/padding_strides_transposed.gif
