.. _conv_arithmetic:

======================
Convolution arithmetic
======================

The relationship between a convolution operation's input shape, kernel size,
stride, padding and its output shape can be confusing at times.

This tutorial aims at providing an intuitive understanding of how the different
convolution parameters influence the shape of the output it produces.

Refresher: discrete convolution
===============================

.. todo::

    WRITEME

Here are the factors determining the shape of a convolution's output size along
a given axis (height or width):

* **Input size**: length of the input axis.
* **Kernel size**: length of the kernel.
* **Stride**: distance between two consecutive positions of the kernel.
* **(Zero-) padding size**: number of zero values concatenated at the beginning
  and at the end of the axis.

For the remainder of this tutorial, we will concentrate on convolutions mapping
one input feature map to one output feature map. The relationships presented
here also hold in the general case.

Since height and width are treated intependently in computing a convolution's
output shape, we will also restrict ourselves to square input, kernel, stride
and padding shapes.

Simplest case
=============

Let's start with the simplest case, where a kernel just slides across the input:

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p = 0` (``border_mode='valid'`` in Theano).

Here is what the convolution looks like for :math:`i = 4` and :math:`k = 3`:

.. figure:: conv_arithmetic_figures/no_padding_no_strides.gif

One way of defining the output size is by the number of possible placements of
the kernel on the input: the left side of the kernel starts at the left side of
the input, and we slide the kernel until its right side touches the right side
of the input. The same applies for the up-down axis.

In doing so, we are counting the number of positions where the bottom right
corner of the kernel can be placed, which is equivalent to ignoring the upmost
:math:`k - 1` rows and the leftmost :math:`k - 1` columns of the input.

From this, we conclude that the output size :math:`o` of the convolution is

.. math::

    o = i - (k - 1).

Zero padding
============

If we factor in zero padding, i.e.

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s = 1`,
* padding size :math:`p`,

all we need to do is to apply the previous rule on the augmented input. Since
padding with :math:`p` zeros adds :math:`2p` to the input size (:math:`p` on
each side), the output size :math:`o` of the convolution becomes

.. math::

    o = i + 2p - (k - 1).

Here is an example for :math:`i = 4`, :math:`k = 4` and :math:`p = 2`:

.. figure:: conv_arithmetic_figures/arbitrary_padding_no_strides.gif

Zero padding: special cases
===========================

There are two special cases of zero padding to take note of, because they're
quite useful in practice.

Same
----

If :math:`k` is odd (:math:`k = 2n + 1`) and
:math:`p = \lfloor k / 2 \rfloor = n`, we have that

.. math::

    o = i + 2 \lfloor k / 2 \rfloor - (k - 1)
      = i + 2n - 2n
      = i.

In other words, the output size is equal to the input size. This is sometimes
referred to as ``'same'`` padding. Here is an example for :math:`i = 3`,
:math:`k = 3` and :math:`p = 1`:

.. figure:: conv_arithmetic_figures/same_padding_no_strides.gif

Full
----

If :math:`p = k - 1`, we have that

.. math::

    o = i + 2(k - 1) - (k - 1)
      = i + (k - 1).

The input size is *increased* by :math:`k - 1` rather than decreased by
:math:`k - 1`. This is sometimes referred to as ``'full'`` padding. Here is
an example for :math:`i = 3`, :math:`k = 3` and :math:`p = 2`:

.. figure:: conv_arithmetic_figures/full_padding_no_strides.gif

Strides
=======

Up until now, we only considered convolutions where the kernel was translated by
increments of :math:`s = 1`. Let's now examine what happens when :math:`s > 1`.
To keep things simple, let's ignore padding, i.e.

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s`,
* padding size :math:`p = 0`.

Here is what the strided convolution looks like for :math:`i = 5`, :math:`k = 3`
and :math:`s = 2`:

.. figure:: conv_arithmetic_figures/no_padding_strides.gif

Once again, we are counting the number of positions where the bottom right
corner of the kernel can be placed. Starting from the bottom right corner of
a kernel placed at the top left corner of the input, we count the number of
:math:`s`-sized hops we can make. This is equivalent to counting the number
of groups of :math:`s` rows and columns we can make when ignoring the first
:math:`k` rows and columns. We then need to add 1, because this counting method
ignores the bottom right corner of the kernel.

From this, we conclude that the output size :math:`o` of the strided convolution
is

.. math::

    o = \left\lfloor \frac{i - k}{s} \right\rfloor + 1.

Combining padding and strides
=============================

The full picture, i.e.

* input size :math:`i`,
* kernel size :math:`k`,
* stride :math:`s`,
* padding size :math:`p`,

is obtained by applying the previous rule on the input augmented by the padding.
In other words,

.. math::

    o = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1.

Here is what it looks like for :math:`i = 5`, :math:`k = 3`, :math:`s = 2` and
:math:`p = 1`:

.. figure:: conv_arithmetic_figures/padding_strides.gif

Transposed convolution: convolution turned on its head
======================================================

Every convolution boils down to an efficient implementation of a matrix
operation. Take for instance the very first convolution presented in this
tutorial. If the input and output were to be unrolled into vectors from left to
right, top to bottom, the matrix representation of the convolution would look
like

.. math::

    \begin{pmatrix}
    w_{0,0} & 0       & 0       & 0       \\
    w_{0,1} & w_{0,0} & 0       & 0       \\
    w_{0,2} & w_{0,1} & 0       & 0       \\
    0       & w_{0,2} & 0       & 0       \\
    w_{1,0} & 0       & w_{0,0} & 0       \\
    w_{1,1} & w_{1,0} & w_{0,1} & w_{0,0} \\
    w_{1,2} & w_{1,1} & w_{0,2} & w_{0,1} \\
    0       & w_{1,2} & 0       & w_{0,2} \\
    w_{2,0} & 0       & w_{1,0} & 0       \\
    w_{2,1} & w_{2,0} & w_{1,1} & w_{1,0} \\
    w_{2,2} & w_{2,1} & w_{1,2} & w_{1,1} \\
    0       & w_{2,2} & 0       & w_{1,2} \\
    0       & 0       & w_{2,0} & 0       \\
    0       & 0       & w_{2,1} & w_{2,0} \\
    0       & 0       & w_{2,2} & w_{2,1} \\
    0       & 0       & 0       & w_{2,2} \\
    \end{pmatrix}^T

where :math:`w_{i,j}` is the element of the kernel located at row :math:`i` and
column :math:`j`. (*Note: the matrix has been transposed for formatting
purposes.*) This linear operation takes a 16-dimensional vector as input and
produces a 4-dimensional vector as output.

What would happen if we were to transpose this matrix and feed the corresponding
linear operation a 4-dimensional vector as input? We would get a 16-dimensional
vector as output. This is what we call a **transposed convolution**.

Simplest case, transposed
=========================

.. figure:: conv_arithmetic_figures/no_padding_no_strides_transposed.gif

Zero padding, transposed
========================

.. figure:: conv_arithmetic_figures/arbitrary_padding_no_strides_transposed.gif

Zero padding, transposed: special cases
=======================================

Same
----

.. figure:: conv_arithmetic_figures/same_padding_no_strides_transposed.gif

Full
----

.. figure:: conv_arithmetic_figures/full_padding_no_strides_transposed.gif

Strides, transposed
===================

.. figure:: conv_arithmetic_figures/no_padding_strides_transposed.gif

Combining padding and strides, transposed
=========================================

.. figure:: conv_arithmetic_figures/padding_strides_transposed.gif
